# k-means++ for Clickhouse SQL


![4clusters](https://github.com/bvt123/clickhouse-k-means/raw/main/4clusters.png?raw=true)

 Делим исходные данные на кластеры, опираясь на поняние "близости" в каком-то многомерном евклидовом пространстве

 - Данных много.
 - Исходые данные могут быть шардированы по серверам (Distributed таблица)
 - результатом должен быть массив PK исходных строк для выявленных кластеров
 
## Зачем clickhouse
 Выгода делать это все не обычным образом (скажем через стандартную библиотеку питона), а внутри КХ на его SQL диалекте заключается в следующем:
- многопоточность по ядрам получается "сама", не нужно ничего специально программировать, КХ будет читать с диска данные в 8 потоков и там-же делать самое сложное вычисление - евклидову дистанцию
- шардинг (если данные уже разложены по серверам) - ускоряет в N раз, так как точки будут обсчитываться  параллельно  и независимо отдельными серверами.
- UDF на питоне тоже решают задачу через 100+ библиотек с доступными реализациям k-means. Только нужно учитывать, что копий питона запускается много, и КХ будет их кормить данными в параллель. Важно чтобы скрипт это учитывал, и работал соответственно.  Получаеся довольно непростое программирование шаред мемори (для хранения центроидов) и прочий непростой interprocess communications.

Идея взята из https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5005&rep=rep1&type=pdf
Но там слишком сложно.  Clickhouse позволяет сделать изящнее.

# Алгоритм

- берем случайную точку как первый центроид
- вычисляем остальные центроиды по алгоритму k-means++
- вычисляем расстояние от каждой точки до всех центроидов
- находим ближайший центроид для каждой точки, объединяем их в группу, назначаем центр этой группы новой позицией центроида
- повторяем пока движение не остановится

# Результат

В таблице WCR лежат строки по одной для каждого центроида/группы, с указанием таймстемпа и массивом PK исходных данных для каждого кластера.
